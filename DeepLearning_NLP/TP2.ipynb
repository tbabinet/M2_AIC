{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# import nltk\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.autograd as ag\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "# reads the content of the file passed as an argument.\n",
    "# if limit > 0, this function will return only the first \"limit\" sentences in the file.\n",
    "def loadTexts(filename, limit=-1):\n",
    "    f = open(filename)\n",
    "    dataset=[]\n",
    "    line =  f.readline()\n",
    "    cpt=1\n",
    "    skip=0\n",
    "    while line :\n",
    "        cleanline = clean_str(f.readline()).split()\n",
    "        if cleanline: \n",
    "            dataset.append(cleanline)\n",
    "        else: \n",
    "            line = f.readline()\n",
    "            skip+=1\n",
    "            continue\n",
    "        if limit > 0 and cpt >= limit: \n",
    "            break\n",
    "        line = f.readline()\n",
    "        cpt+=1        \n",
    "        \n",
    "    f.close()\n",
    "    print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  5000  lines from  imdb.pos  /  1  lines discarded\n",
      "Load  5000  lines from  imdb.neg  /  1  lines discarded\n"
     ]
    }
   ],
   "source": [
    "LIM=5000\n",
    "txtfile = \"imdb.pos\"  # path of the file containing positive reviews\n",
    "postxt = loadTexts(txtfile,limit=LIM)\n",
    "\n",
    "txtfile = \"imdb.neg\"  # path of the file containing negative reviews\n",
    "negtxt = loadTexts(txtfile,limit=LIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = postxt[:int(len(postxt)*0.8)]+negtxt[:int(len(negtxt)*0.8)]\n",
    "train_label = np.array([1 for i in range(int(len(train_data)/2))]+[0 for i in range(int(len(train_data)/2))], dtype=np.float32)\n",
    "\n",
    "dev_data =postxt[int(len(postxt)*0.8):int(len(postxt)*0.9)]+negtxt[int(len(postxt)*0.8):int(len(negtxt)*0.9)]\n",
    "dev_label = np.array([1 for i in range(int(len(dev_data)/2))]+[0 for i in range(int(len(dev_data)/2))], dtype=np.float32)\n",
    "\n",
    "test_data = postxt[int(len(postxt)*0.9):]+negtxt[int(len(postxt)*0.9):]  \n",
    "test_label = np.array([1 for i in range(int(len(test_data)/2))]+[0 for i in range(int(len(test_data)/2))], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size :  6999\n"
     ]
    }
   ],
   "source": [
    "train_txt = \"\"\n",
    "for s in train_data:\n",
    "    for w in s:\n",
    "        train_txt+= w +\" \"\n",
    "        \n",
    "token = train_txt.lower().split()\n",
    "words = Counter(token)\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "words = ['_PAD','_UNK'] + words\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "vocab_size = len(word2idx)\n",
    "print(\"vocab size : \",vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [[word2idx[w] for w in s] for s in train_data]\n",
    "dev_data = [[word2idx[w] if w in word2idx else 0 for w in s] for s in dev_data]\n",
    "test_data = [[word2idx[w] if w in word2idx else 0 for w in s] for s in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36, 26, 742, 11, 63, 22, 80]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_input(data, seq_len):\n",
    "    features = np.zeros((len(data), seq_len), dtype=int)\n",
    "    for ii, review in enumerate(data):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = max([len(s) for s in train_data])\n",
    "\n",
    "train_data = pad_input(train_data, seq_len)\n",
    "dev_data = pad_input(dev_data, seq_len)\n",
    "test_data = pad_input(dev_data, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(th.from_numpy(train_data), th.from_numpy(train_label))\n",
    "dev_data = TensorDataset(th.from_numpy(dev_data), th.from_numpy(dev_label))\n",
    "test_data = TensorDataset(th.from_numpy(test_data), th.from_numpy(test_label))\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "dev_loader = DataLoader(dev_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, kernel_size, feat_size):\n",
    "        super(Conv_classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.conv1 = nn.Linear(embedding_dim*kernel_size, feat_size)\n",
    "        self.l1 = nn.Linear(feat_size, 1)\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        nn.init.xavier_uniform_(self.l1.weight.data)  # Xavier/Glorot init for tanh\n",
    "        nn.init.zeros_(self.l1.bias.data)  # Xavier/Glorot init for tanh\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.shape[0]\n",
    "#         inputs = inputs.view(batch_size, inputs.shape[1], 1)\n",
    "        \n",
    "#         if(inputs.shape[0]<self.kernel_size):\n",
    "#             pad = th.tensor([0 for i in range(self.kernel_size-inputs.shape[0])])\n",
    "#             inputs = th.cat((inputs, pad), 0)\n",
    "\n",
    "        inputs = self.embedding(inputs)\n",
    "        \n",
    "        cat_inputs= []\n",
    "        h = th.Tensor(batch_size,inputs.shape[1], inputs.shape[2]*self.kernel_size)\n",
    "        \n",
    "        for i in range(len(inputs)-self.kernel_size+1):\n",
    "            tab = [inputs[:,j,:] for j in range(i, self.kernel_size+i)]\n",
    "            cat = th.cat(tab, 1)\n",
    "            cat_inputs.append(self.conv1(cat))\n",
    "        \n",
    "        h = th.stack(cat_inputs)\n",
    "        h, _ = th.max(h, 0)\n",
    "        \n",
    "        out = self.l1(h)\n",
    "        out = out.view(-1)\n",
    "        return th.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim,nb_cell, hidden_dim, feat_size):\n",
    "        super(LSTM_Classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.nb_cell = nb_cell\n",
    "        \n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim, nb_cells, batch_first = True)\n",
    "        self.l1 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        \n",
    "        nn.init.xavier_uniform_(self.l1.weight.data)  # Xavier/Glorot init for tanh\n",
    "        nn.init.zeros_(self.l1.bias.data)  # Xavier/Glorot init for tanh\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        \n",
    "        inputs = self.embedding(inputs)\n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = th.Tensor(self.nb_cells, batch_size, self.hidden_size)\n",
    "        if self.is_lstm:\n",
    "            hidden = (weight.new(self.nb_cells, batch_size, self.hidden_size).zero_(),\n",
    "                      weight.new(self.nb_cells, batch_size, self.hidden_size).zero_())\n",
    "        else:\n",
    "            hidden = weight.new(self.nb_cells, batch_size, self.hidden_size).zero_()\n",
    "            \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "lr = 1e-1\n",
    "m = Conv_classifier(len(word2idx), 5, 3, 5)\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "dev_accuracies = []\n",
    "dev_losses = []\n",
    "clip=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, max_epochs=20):\n",
    "    optim = th.optim.SGD(params=model.parameters(), lr =lr, weight_decay=1e-4)\n",
    "#     model.train()\n",
    "\n",
    "    \n",
    "    for e in range(max_epochs):\n",
    "        train_accuracy = 0\n",
    "        dev_accuracy = 0\n",
    "        train_mean_loss = 0\n",
    "        dev_mean_loss = 0\n",
    "        \n",
    "        \n",
    "        for x,labels in train_loader:\n",
    "\n",
    "            preds = model(x)\n",
    "            \n",
    "            loss = loss_fn(preds, labels)\n",
    "            train_mean_loss+=loss.item()\n",
    "            optim.zero_grad()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            for pred, label in zip(preds, labels):\n",
    "                if (pred<0.5 and label==0) or (pred>0.5 and label==1):\n",
    "                    train_accuracy+=1\n",
    "            \n",
    "                \n",
    "        ###Dev test\n",
    "        for x, labels in dev_loader:\n",
    "            preds = model(x)\n",
    "            loss = loss_fn(preds, labels)\n",
    "            dev_mean_loss+=loss.item()\n",
    "            \n",
    "            for pred, label in zip(preds, labels):\n",
    "                if (pred<0.5 and label==0) or (pred>0.5 and label==1):\n",
    "                    dev_accuracy+=1\n",
    "                    \n",
    "        train_accuracies.append(train_accuracy/len(train_data))\n",
    "        train_losses.append(train_mean_loss/len(train_data))\n",
    "        dev_accuracies.append(dev_accuracy/len(dev_data))\n",
    "        dev_losses.append(dev_mean_loss/len(dev_data))\n",
    "        print(\"EPOCH {}\".format(e+1))\n",
    "        print(\"Train Accuracy : \",train_accuracy/len(train_data))\n",
    "        print(\"Dev Accuracy : \",dev_accuracy/len(dev_data))\n",
    "        print(\"Train Mean loss : \",train_mean_loss/len(train_data))\n",
    "        print(\"Dev Mean loss : \",dev_mean_loss/len(dev_data))\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "Train Accuracy :  0.53775\n",
      "Dev Accuracy :  0.557\n",
      "Train Mean loss :  0.013837529040873051\n",
      "Dev Mean loss :  0.013682193636894225\n",
      "----------------------------------------\n",
      "EPOCH 2\n",
      "Train Accuracy :  0.558375\n",
      "Dev Accuracy :  0.582\n",
      "Train Mean loss :  0.013676310524344443\n",
      "Dev Mean loss :  0.013619078636169433\n",
      "----------------------------------------\n",
      "EPOCH 3\n",
      "Train Accuracy :  0.567125\n",
      "Dev Accuracy :  0.583\n",
      "Train Mean loss :  0.01362375996261835\n",
      "Dev Mean loss :  0.013570053398609162\n",
      "----------------------------------------\n",
      "EPOCH 4\n",
      "Train Accuracy :  0.56925\n",
      "Dev Accuracy :  0.589\n",
      "Train Mean loss :  0.013579626128077508\n",
      "Dev Mean loss :  0.01352558070421219\n",
      "----------------------------------------\n",
      "EPOCH 5\n",
      "Train Accuracy :  0.57875\n",
      "Dev Accuracy :  0.594\n",
      "Train Mean loss :  0.013543844170868397\n",
      "Dev Mean loss :  0.013506351232528687\n",
      "----------------------------------------\n",
      "EPOCH 6\n",
      "Train Accuracy :  0.580125\n",
      "Dev Accuracy :  0.582\n",
      "Train Mean loss :  0.013473866969347\n",
      "Dev Mean loss :  0.013470309495925903\n",
      "----------------------------------------\n",
      "EPOCH 7\n",
      "Train Accuracy :  0.592875\n",
      "Dev Accuracy :  0.579\n",
      "Train Mean loss :  0.013422070421278477\n",
      "Dev Mean loss :  0.01338275408744812\n",
      "----------------------------------------\n",
      "EPOCH 8\n",
      "Train Accuracy :  0.591125\n",
      "Dev Accuracy :  0.603\n",
      "Train Mean loss :  0.013387138925492764\n",
      "Dev Mean loss :  0.013336483418941497\n",
      "----------------------------------------\n",
      "EPOCH 9\n",
      "Train Accuracy :  0.598875\n",
      "Dev Accuracy :  0.608\n",
      "Train Mean loss :  0.013315054945647716\n",
      "Dev Mean loss :  0.013314951717853546\n",
      "----------------------------------------\n",
      "EPOCH 10\n",
      "Train Accuracy :  0.600625\n",
      "Dev Accuracy :  0.594\n",
      "Train Mean loss :  0.013262664049863815\n",
      "Dev Mean loss :  0.013247909426689148\n",
      "----------------------------------------\n",
      "EPOCH 11\n",
      "Train Accuracy :  0.608625\n",
      "Dev Accuracy :  0.618\n",
      "Train Mean loss :  0.013211114160716533\n",
      "Dev Mean loss :  0.013236510992050171\n",
      "----------------------------------------\n",
      "EPOCH 12\n",
      "Train Accuracy :  0.6125\n",
      "Dev Accuracy :  0.573\n",
      "Train Mean loss :  0.013142763286828995\n",
      "Dev Mean loss :  0.013574624121189118\n",
      "----------------------------------------\n",
      "EPOCH 13\n",
      "Train Accuracy :  0.61925\n",
      "Dev Accuracy :  0.602\n",
      "Train Mean loss :  0.013078036054968835\n",
      "Dev Mean loss :  0.013192289650440217\n",
      "----------------------------------------\n",
      "EPOCH 14\n",
      "Train Accuracy :  0.6175\n",
      "Dev Accuracy :  0.592\n",
      "Train Mean loss :  0.013007592953741551\n",
      "Dev Mean loss :  0.013145965814590455\n",
      "----------------------------------------\n",
      "EPOCH 15\n",
      "Train Accuracy :  0.62575\n",
      "Dev Accuracy :  0.604\n",
      "Train Mean loss :  0.012900383435189724\n",
      "Dev Mean loss :  0.012942256689071655\n",
      "----------------------------------------\n",
      "EPOCH 16\n",
      "Train Accuracy :  0.629125\n",
      "Dev Accuracy :  0.618\n",
      "Train Mean loss :  0.012815827243030072\n",
      "Dev Mean loss :  0.012910487651824952\n",
      "----------------------------------------\n",
      "EPOCH 17\n",
      "Train Accuracy :  0.630375\n",
      "Dev Accuracy :  0.625\n",
      "Train Mean loss :  0.01278081265091896\n",
      "Dev Mean loss :  0.012811479032039642\n",
      "----------------------------------------\n",
      "EPOCH 18\n",
      "Train Accuracy :  0.637375\n",
      "Dev Accuracy :  0.624\n",
      "Train Mean loss :  0.012646446950733662\n",
      "Dev Mean loss :  0.012817225873470306\n",
      "----------------------------------------\n",
      "EPOCH 19\n",
      "Train Accuracy :  0.63825\n",
      "Dev Accuracy :  0.634\n",
      "Train Mean loss :  0.01259102513641119\n",
      "Dev Mean loss :  0.012750103354454041\n",
      "----------------------------------------\n",
      "EPOCH 20\n",
      "Train Accuracy :  0.639875\n",
      "Dev Accuracy :  0.634\n",
      "Train Mean loss :  0.012506940856575966\n",
      "Dev Mean loss :  0.012671383440494538\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
