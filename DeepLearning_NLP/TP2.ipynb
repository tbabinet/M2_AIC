{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# import nltk\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.autograd as ag\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "# reads the content of the file passed as an argument.\n",
    "# if limit > 0, this function will return only the first \"limit\" sentences in the file.\n",
    "def loadTexts(filename, limit=-1):\n",
    "    f = open(filename)\n",
    "    dataset=[]\n",
    "    line =  f.readline()\n",
    "    cpt=1\n",
    "    skip=0\n",
    "    while line :\n",
    "        cleanline = clean_str(f.readline()).split()\n",
    "        if cleanline: \n",
    "            dataset.append(cleanline)\n",
    "        else: \n",
    "            line = f.readline()\n",
    "            skip+=1\n",
    "            continue\n",
    "        if limit > 0 and cpt >= limit: \n",
    "            break\n",
    "        line = f.readline()\n",
    "        cpt+=1        \n",
    "        \n",
    "    f.close()\n",
    "    print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  5000  lines from  imdb.pos  /  1  lines discarded\n",
      "Load  5000  lines from  imdb.neg  /  1  lines discarded\n"
     ]
    }
   ],
   "source": [
    "LIM=5000\n",
    "txtfile = \"imdb.pos\"  # path of the file containing positive reviews\n",
    "postxt = loadTexts(txtfile,limit=LIM)\n",
    "\n",
    "txtfile = \"imdb.neg\"  # path of the file containing negative reviews\n",
    "negtxt = loadTexts(txtfile,limit=LIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = postxt[:int(len(postxt)*0.8)]+negtxt[:int(len(negtxt)*0.8)]\n",
    "train_label = np.array([1.0 for i in range(int(len(train_data)/2))]+[0.0 for i in range(int(len(train_data)/2))])\n",
    "\n",
    "dev_data =postxt[int(len(postxt)*0.8):int(len(postxt)*0.9)]+negtxt[int(len(postxt)*0.8):int(len(negtxt)*0.9)]\n",
    "dev_label = np.array([1.0 for i in range(int(len(dev_data)/2))]+[0.0 for i in range(int(len(dev_data)/2))])\n",
    "\n",
    "test_data = postxt[int(len(postxt)*0.9):]+negtxt[int(len(postxt)*0.9):]  \n",
    "test_label = np.array([1.0 for i in range(int(len(test_data)/2))]+[0.0 for i in range(int(len(test_data)/2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size :  6997\n"
     ]
    }
   ],
   "source": [
    "train_txt = \"\"\n",
    "for s in train_data:\n",
    "    for w in s:\n",
    "        train_txt+= w +\" \"\n",
    "        \n",
    "token = train_txt.lower().split()\n",
    "words = Counter(token)\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "vocab_size = len(words)\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "word2idx[\"UNK\"]=len(word2idx)\n",
    "print(\"vocab size : \",vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, kernel_size, feat_size):\n",
    "        super(Conv_classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.conv1 = nn.Linear(embedding_dim*kernel_size, feat_size)\n",
    "        self.l1 = nn.Linear(feat_size, 1)\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        nn.init.xavier_uniform_(self.l1.weight.data)  # Xavier/Glorot init for tanh\n",
    "        nn.init.zeros_(self.l1.bias.data)  # Xavier/Glorot init for tanh\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        if(inputs.shape[0]<self.kernel_size):\n",
    "            pad = th.tensor([0 for i in range(self.kernel_size-inputs.shape[0])])\n",
    "            inputs = th.cat((inputs, pad), 0)\n",
    "        \n",
    "        inputs = self.embedding(inputs)\n",
    "        cat_inputs= []\n",
    "        h = th.Tensor(inputs.shape[0], inputs.shape[1]*self.kernel_size)\n",
    "        \n",
    "        for i in range(len(inputs)-self.kernel_size+1):\n",
    "            tab = [inputs[j] for j in range(i, self.kernel_size+i)]\n",
    "            cat = th.cat(tab, 0)\n",
    "            cat_inputs.append(self.conv1(cat))\n",
    "        \n",
    "        h = th.stack(cat_inputs)\n",
    "        h, _ = th.max(h, 0)\n",
    "        \n",
    "        out = self.l1(h)\n",
    "        return th.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "lr = 1e-1\n",
    "m = Conv_classifier(len(word2idx), 5, 3, 5)\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "dev_accuracies = []\n",
    "dev_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, max_epochs=20):\n",
    "    optim = th.optim.SGD(params=model.parameters(), lr =lr, weight_decay=1e-4)\n",
    "#     model.train()\n",
    "    idx_train = np.arange(len(train_data))\n",
    "    idx_dev = np.arange(len(dev_data))\n",
    "    \n",
    "    for e in range(max_epochs):\n",
    "        train_accuracy = 0\n",
    "        dev_accuracy = 0\n",
    "        train_mean_loss = 0\n",
    "        dev_mean_loss = 0\n",
    "        \n",
    "        np.random.shuffle(idx_train)\n",
    "        np.random.shuffle(idx_dev)\n",
    "        for i in idx_train:\n",
    "            s = train_data[i]\n",
    "            y = train_label[i]\n",
    "            \n",
    "            idxsentence = th.LongTensor([word2idx[w] for w in s])\n",
    "            label = th.tensor([y])\n",
    "            pred = model(idxsentence)\n",
    "            loss = loss_fn(pred, label)\n",
    "\n",
    "            train_mean_loss+=loss.item()\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            if((pred<0.5 and label==0) or (pred>0.5 and label == 1)):\n",
    "                train_accuracy+=1\n",
    "        ###Dev test\n",
    "        for i in idx_dev:\n",
    "            s = dev_data[i]\n",
    "            y = dev_label[i]\n",
    "            idxsentence = th.LongTensor([word2idx[w] for w in s if w in word2idx])\n",
    "            label = th.tensor([y])\n",
    "            pred = model(idxsentence)\n",
    "            loss = loss_fn(pred, label)\n",
    "            dev_mean_loss+=loss.item()\n",
    "            if((pred<0.5 and label==0) or (pred>0.5 and label == 1)):\n",
    "                dev_accuracy+=1\n",
    "        train_accuracies.append(train_accuracy/len(train_data))\n",
    "        train_losses.append(train_mean_loss/len(train_data))\n",
    "        dev_accuracies.append(dev_accuracy/len(dev_data))\n",
    "        dev_losses.append(dev_mean_loss/len(dev_data))\n",
    "        print(\"EPOCH {}\".format(e+1))\n",
    "        print(\"Train Accuracy : \",train_accuracy/len(train_data))\n",
    "        print(\"Dev Accuracy : \",dev_accuracy/len(dev_data))\n",
    "        print(\"Train Mean loss : \",train_mean_loss/len(train_data))\n",
    "        print(\"Dev Mean loss : \",dev_mean_loss/len(dev_data))\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "Train Accuracy :  0.5055\n",
      "Dev Accuracy :  0.523\n",
      "Train Mean loss :  0.7166379799523857\n",
      "Dev Mean loss :  0.7075098401606082\n",
      "----------------------------------------\n",
      "EPOCH 2\n",
      "Train Accuracy :  0.538125\n",
      "Dev Accuracy :  0.533\n",
      "Train Mean loss :  0.7055273558418267\n",
      "Dev Mean loss :  0.680806639611721\n",
      "----------------------------------------\n",
      "EPOCH 3\n",
      "Train Accuracy :  0.6165\n",
      "Dev Accuracy :  0.644\n",
      "Train Mean loss :  0.656465456299693\n",
      "Dev Mean loss :  0.6737398589443182\n",
      "----------------------------------------\n",
      "EPOCH 4\n",
      "Train Accuracy :  0.687625\n",
      "Dev Accuracy :  0.68\n",
      "Train Mean loss :  0.5847738357541057\n",
      "Dev Mean loss :  0.6046349857150563\n",
      "----------------------------------------\n",
      "EPOCH 5\n",
      "Train Accuracy :  0.729\n",
      "Dev Accuracy :  0.679\n",
      "Train Mean loss :  0.5271868862870419\n",
      "Dev Mean loss :  0.5939605705873546\n",
      "----------------------------------------\n",
      "EPOCH 6\n",
      "Train Accuracy :  0.762\n",
      "Dev Accuracy :  0.737\n",
      "Train Mean loss :  0.48723723436920024\n",
      "Dev Mean loss :  0.5203046619867964\n",
      "----------------------------------------\n",
      "EPOCH 7\n",
      "Train Accuracy :  0.791875\n",
      "Dev Accuracy :  0.731\n",
      "Train Mean loss :  0.44295761901643627\n",
      "Dev Mean loss :  0.507657536917366\n",
      "----------------------------------------\n",
      "EPOCH 8\n",
      "Train Accuracy :  0.813625\n",
      "Dev Accuracy :  0.746\n",
      "Train Mean loss :  0.4021978111838586\n",
      "Dev Mean loss :  0.5395445430835444\n",
      "----------------------------------------\n",
      "EPOCH 9\n",
      "Train Accuracy :  0.836125\n",
      "Dev Accuracy :  0.741\n",
      "Train Mean loss :  0.36795777880517166\n",
      "Dev Mean loss :  0.5926918344385201\n",
      "----------------------------------------\n",
      "EPOCH 10\n",
      "Train Accuracy :  0.85275\n",
      "Dev Accuracy :  0.745\n",
      "Train Mean loss :  0.3327602000818226\n",
      "Dev Mean loss :  0.5828643516588965\n",
      "----------------------------------------\n",
      "EPOCH 11\n",
      "Train Accuracy :  0.877875\n",
      "Dev Accuracy :  0.755\n",
      "Train Mean loss :  0.3043543000317108\n",
      "Dev Mean loss :  0.5878150600783382\n",
      "----------------------------------------\n",
      "EPOCH 12\n",
      "Train Accuracy :  0.882875\n",
      "Dev Accuracy :  0.746\n",
      "Train Mean loss :  0.2806234082278219\n",
      "Dev Mean loss :  0.7718166799568901\n",
      "----------------------------------------\n",
      "EPOCH 13\n",
      "Train Accuracy :  0.902125\n",
      "Dev Accuracy :  0.739\n",
      "Train Mean loss :  0.26258514494121865\n",
      "Dev Mean loss :  0.6628407890019395\n",
      "----------------------------------------\n",
      "EPOCH 14\n",
      "Train Accuracy :  0.907625\n",
      "Dev Accuracy :  0.741\n",
      "Train Mean loss :  0.24761313358657794\n",
      "Dev Mean loss :  0.6636787014901896\n",
      "----------------------------------------\n",
      "EPOCH 15\n",
      "Train Accuracy :  0.912375\n",
      "Dev Accuracy :  0.75\n",
      "Train Mean loss :  0.2301045078223152\n",
      "Dev Mean loss :  0.6203263186692745\n",
      "----------------------------------------\n",
      "EPOCH 16\n",
      "Train Accuracy :  0.918\n",
      "Dev Accuracy :  0.723\n",
      "Train Mean loss :  0.21972950142848222\n",
      "Dev Mean loss :  0.8172477320651131\n",
      "----------------------------------------\n",
      "EPOCH 17\n",
      "Train Accuracy :  0.925875\n",
      "Dev Accuracy :  0.732\n",
      "Train Mean loss :  0.20958922904684382\n",
      "Dev Mean loss :  0.8703275395877199\n",
      "----------------------------------------\n",
      "EPOCH 18\n",
      "Train Accuracy :  0.9305\n",
      "Dev Accuracy :  0.731\n",
      "Train Mean loss :  0.20654140751489297\n",
      "Dev Mean loss :  0.6029146278908015\n",
      "----------------------------------------\n",
      "EPOCH 19\n",
      "Train Accuracy :  0.929875\n",
      "Dev Accuracy :  0.739\n",
      "Train Mean loss :  0.20226828235586428\n",
      "Dev Mean loss :  0.8082980297593344\n",
      "----------------------------------------\n",
      "EPOCH 20\n",
      "Train Accuracy :  0.931125\n",
      "Dev Accuracy :  0.732\n",
      "Train Mean loss :  0.19126406290765943\n",
      "Dev Mean loss :  0.9023148869296036\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
