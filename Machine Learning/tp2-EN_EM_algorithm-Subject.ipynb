{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EM Algorithm** (Expectation - Maximization)\n",
    "\n",
    "We're going to implement the EM algorithm for a mixture of Bernoullis\n",
    "\n",
    "The Expectation-Maximization algo. is used in sk-learn, for instance in GMMs: http://scikit-learn.org/stable/modules/mixture.html#estimation-algorithm-expectation-maximization\n",
    "\n",
    "Additional notes are available here: https://allauzen.github.io/articles/MixturesAndEM/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Think for a bit\n",
    "\n",
    "Is EM a supervised or unsupervised learning algorithm ? \n",
    "\n",
    "What kind of data set is MNIST ? \n",
    "\n",
    "Then, what are we going to do ?  How is this called ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Implement the EM algo. for a mixture of Bernoulli (laws)\n",
    "- The cluster number K should be an argument of the function\n",
    "- A maximum number of iterations, *MaxIt*, should act as stopping condition\n",
    "- During the E step, compute and *store* the log-likelihood of the data, so as to monitor its evolution along iterations (~epochs)\n",
    "\n",
    "Apply the algorithm on MNIST:\n",
    "- try out K=5,10,15\n",
    "- Visualize the images coresponding to each cluster's paraemeters. Would that be as straightforward in a Gaussian model (visualizing all the model's parameters?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we load the whole data set at once, so as to manipulate it directly (using numpy arrays)\n",
    "## for very large data sets, one needs to read on-the-fly (at least at production time, not at the debugging stage)\n",
    "\n",
    "# ## Load the dataset -- (python2)\n",
    "# import cPickle, gzip\n",
    "# with gzip.open('./tp1-mnist.pkl.gz','rb') as f :\n",
    "#     train_set, valid_set, test_set = cPickle.load(f)\n",
    "\n",
    "### Load the dataset -- (python3)\n",
    "import pickle, gzip\n",
    "with gzip.open('mnist.pkl.gz','rb') as f :\n",
    "    u = pickle._Unpickler( f )\n",
    "    u.encoding = 'latin1'\n",
    "    train_set, valid_set, test_set = u.load()\n",
    "\n",
    "## split train an test data, to avoid inadvertently cheating.\n",
    "unlabelled_dataset = train_set[0].copy()\n",
    "labels_for_final_accuracy_measurement = train_set[1].copy()\n",
    "del train_set\n",
    "\n",
    "## these would be useful, for now we don't use them\n",
    "del valid_set  \n",
    "del test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## binarize so as to go into Bernoulli-space\n",
    "threshold = 0.3\n",
    "for i in range(unlabelled_dataset.shape[0]): \n",
    "    unlabelled_dataset[i,:]= 1.0*(unlabelled_dataset[i]>threshold)\n",
    "## (unlabelled_dataset[i]>threshold) is now an array of Booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## inspect the new values \"by hand\", to check.\n",
    "unlabelled_dataset[0][300:400] ## reading \n",
    "## we do NOT convert to Integers, instead we stay in floats. Because we'll compute averages later (thus, divide..)\n",
    "sum(unlabelled_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for debugging, we may want to use a small piece of the sample\n",
    "subSampling = 100  ## data size will be divided by a factor \"subSampling\"\n",
    "## in the end, we may use all of it (set this to 1) \n",
    "\n",
    "## sort data to make sure all classes are equally present ##\n",
    "ordre = np.argsort(labels_for_final_accuracy_measurement)\n",
    "unlabelled_dataset = unlabelled_dataset[ordre]\n",
    "labels_for_final_accuracy_measurement = labels_for_final_accuracy_measurement[ordre]\n",
    "\n",
    "## do the sub-sampling\n",
    "dataset = unlabelled_dataset[::subSampling].copy()\n",
    "subSampledLabels = labels_for_final_accuracy_measurement[::subSampling].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check it:\n",
    "subSampledLabels[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-327116066a8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0moutputForDebug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_P_Xi_given_k_and_theta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'xi' is not defined"
     ]
    }
   ],
   "source": [
    "## when you code, start by writing on a copy of the data \n",
    "## (not the data themselves: otherwise on the second run of the jupyter cell, you'll have run twice)\n",
    "K=10\n",
    "MaxIt=4\n",
    "\n",
    "## compute the proba of an image xi, \n",
    "## given its cluster k and parameters theta \n",
    "def compute_P_Xi_given_k_and_theta(xi, k, theta):\n",
    "    ## TODO \n",
    "    return output\n",
    "\n",
    "outputForDebug = compute_P_Xi_given_k_and_theta(xi, k, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python-note: at debug time, don't write a function, instead, write directly in the main\n",
    "## when everything works well, encapsulate the piece of code into a function for later re-use (and lisibility)\n",
    "#def AlgoEM(dataset, K, MaxIt, labels_for_final_accuracy_measurement):\n",
    "\n",
    "## we set the seed to a constant so as to have repeatable experiences \n",
    "## (at debug time, and when comparing hyper-parameters)\n",
    "np.random.seed(42)\n",
    "\n",
    "## we'll assume dataset[i,j] is the value of pixel j of image i.\n",
    "Nex = (dataset.shape)[0] ## number of examples (= number of images)\n",
    "dim = (dataset.shape)[1] ## dimension of parameters space (=number of pixels per image, in the Bernoulli case)\n",
    "classFrequencies = np.zeros(K) ## denoted pi_k \n",
    "theta = np.zeros((K, dim))   ## denoted theta_{k,j}\n",
    "\n",
    "## initialization ##\n",
    "affectations = np.random.random((K, Nex)) ## denoted a_{k,i}\n",
    "## normalization step (needed to make the )\n",
    "for i in range(Nex):\n",
    "    affectations[:,i] /= np.sum(affectations[:,i])\n",
    "    \n",
    "## main loop ##\n",
    "for iteration in range(0,MaxIt,1):\n",
    "    # if iteration%10 == 0:\n",
    "    #     print(\"iteration numero\"+str(iteration))\n",
    "\n",
    "    ######################\n",
    "    ## step M: update of \"pi_k, mu_k\" (classFrequencies, theta)  ##\n",
    "    ## TODO \n",
    "\n",
    "    ######################\n",
    "    ## step E: update of \"a_ik\" (affectations) ##\n",
    "    ## TODO \n",
    "\n",
    "    ## monitoring of the quality of te clustering ##\n",
    "    LogLikelihood = np.log(np.max(affectations, axis=0)) \n",
    "    print(np.mean(LogLikelihood))\n",
    "    ## TODO: record the monitoring into an array, to be able to later plot it\n",
    "\n",
    "#    return affectations, theta, classFrequencies\n",
    "##########################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classFrequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LogVraisemblance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can visualize the parameters theta, cluster by cluster\n",
    "for k in range(K):\n",
    "    plt.imshow(theta[k].reshape(28,28) , matplotlib.pyplot.cm.jet)\n",
    "    matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: after checking your code works, put it inside a function and call that function \n",
    " (instead of running code in the *main()* directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affectations, theta, classFrequencies = AlgoEM(dataset, K, MaxIt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's monitor something more interpretable than the log-likelihood... what could it be ? \n",
    "\n",
    "Trick: you may use the function np.argmax(), and maybe just a couple of labels from a couple of examples\n",
    "\n",
    "**Quesiton**: would this strategy be possible for a purely unsupervised task ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: (later) \n",
    "## what , 0,1,2,...9\n",
    "# prediction = np.argmax(affectations, axis=0)\n",
    "# score = np.mean(prediction == labels_for_final_accuracy_measurement[:subSampling])\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add this interesting metric into your function, and add some plot of it from the function as well (possibly along wth the log-likelihood plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you may play with the hyper-parameters.\n",
    "\n",
    "What is the best value of the main hyper-parameter ? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
