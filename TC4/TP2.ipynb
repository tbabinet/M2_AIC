{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IwILkW1F8FnJ"
   },
   "source": [
    "# TC4 - Lab exercises 2\n",
    "\n",
    "In this notebook, we are going to improve the POS tagger of last week. Instead of using a naive Bayes classifier, we will rely on a HMM where:\n",
    "- the hidden states are POS tags\n",
    "- the observations are words\n",
    "\n",
    "It is a first order HMM where probabilities are defined as follows:\n",
    "$$\n",
    "p(y_1...y_n, x_1...x_n) = p(y_1) \\prod_{i=2}^n p(y_i | y_{i-1}) \\prod_{i=1}^n p(x_i | y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IwILkW1F8FnJ"
   },
   "source": [
    "# 1. Data\n",
    "\n",
    "We first need to load and split the data between train and test data. You need to report the code from last week with the same split (90% train / 10% test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51606, 5734)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store the parameters of the HMM in numpy arrays. Therefore, to simplify the model, we rely on a dictionnary that maps words to tokens:\n",
    "- the constructor takes as argument an iteratable over strings (e.g. list of strings) containing the vocabulary to store in the dictionnary\n",
    "- you can set unk=\"\\*UNK\\*\" to add entry for unknown strings (do not do it for POS tags!)\n",
    "- len(dict) gives you the numbers of entry in the dict\n",
    "- str_to_id maps a string to an index\n",
    "- id_to_str gives you the string stored at a given index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dict:\n",
    "    def __init__(self, words, unk=None):\n",
    "        self._unk = unk\n",
    "        self._word_to_id = dict()\n",
    "        self._id_to_word = list()\n",
    "\n",
    "        if unk in words:\n",
    "            raise RuntimeError(\"UNK word exists in vocabulary\")\n",
    "\n",
    "        if unk is not None:\n",
    "            self.unk_index = self._add_word(unk)\n",
    "\n",
    "        for word in words:\n",
    "            self._add_word(word)\n",
    "\n",
    "    # for internal use only!\n",
    "    def _add_word(self, word):\n",
    "        if word not in self._word_to_id:\n",
    "            id = len(self._id_to_word)\n",
    "            self._word_to_id[word] = id\n",
    "            self._id_to_word.append(word)\n",
    "            return id\n",
    "        else:\n",
    "            return self._word_to_id[word]\n",
    "\n",
    "    def str_to_id(self, word):\n",
    "        if self._unk is not None:\n",
    "            return self._word_to_id.get(word, self.unk_index)\n",
    "        else:\n",
    "            return self._word_to_id[word]\n",
    "\n",
    "    def id_to_str(self, id):\n",
    "        return self._id_to_word[id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._word_to_id)\n",
    "\n",
    "    def has_unk(self):\n",
    "        return self._unk is not None\n",
    "    \n",
    "    def unk(self):\n",
    "        return self.unk_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N. entry:  4\n",
      "Index of \"b\": 1\n",
      "Index of \"e\": 0\n"
     ]
    }
   ],
   "source": [
    "test_dict = Dict([\"a\", \"b\", \"c\"], unk=\"*UNK*\")\n",
    "print(\"N. entry: \", len(test_dict))\n",
    "print(\"Index of \\\"b\\\":\", test_dict.str_to_id(\"a\"))\n",
    "# the following line does not throw an error because we gave a unk word\n",
    "print(\"Index of \\\"e\\\":\", test_dict.str_to_id(\"e\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build the dictionnary of words and tags. We will restrict the dictionnary of words to contain only words that appears 10 or more times in the training data (use the code of last time).\n",
    "\n",
    "For the dictionnary of words, set the unk parameters to any string you want. For the dictionnary of POS tags, do not set an unk word!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_per_word = {}\n",
    "distribution_per_word_correct = {}\n",
    "\n",
    "for s in train_data:\n",
    "    for w, tag in s:\n",
    "        if w in distribution_per_word : \n",
    "            if tag in distribution_per_word[w]:\n",
    "                distribution_per_word[w][tag]+=1\n",
    "            else:\n",
    "                distribution_per_word[w][tag] = 1\n",
    "        else:\n",
    "            distribution_per_word[w]={}\n",
    "            distribution_per_word[w][tag] = 1\n",
    "\n",
    "for w in distribution_per_word:\n",
    "    total = sum(distribution_per_word[w].values())\n",
    "    if (total>=10): \n",
    "        distribution_per_word_correct[w] = distribution_per_word[w]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [list(x.keys()) for x in distribution_per_word_correct.values()]\n",
    "\n",
    "tags = [item for sublist in tmp for item in sublist]\n",
    "\n",
    "word_dict = Dict(distribution_per_word_correct.keys(), unk = \"**UNK**\")\n",
    "tags_dict = Dict(tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hidden Markov Model\n",
    "\n",
    "The HMM class is a simple container for:\n",
    "- the dictionnary of hidden states y_dict (i.e. dictionnary of tags)\n",
    "- the dictionnary of observations x_dict (i.e. dictionnary of words)\n",
    "- the parameters of the HMM:\n",
    "    * init_prob $\\in \\mathbb R^{|Y|}$: initial tag probabilities $p(y_0) = init\\_prob[y_0]$\n",
    "    * transition_prob $\\in \\mathbb R^{|Y| \\times |Y|}$: tag transition probabilities $p(y_i | y_{i - 1}) = transition\\_prob[y_{i - 1}, y_i]$\n",
    "    * observation_prob $\\in \\mathbb R^{|Y| \\times |X|}$: observation probabilities $p(x_i | y_i) = observation\\_prob[y_i, x_i]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtP9d0Pz8FnL"
   },
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, y_dict, x_dict):\n",
    "        if not isinstance(y_dict, Dict) or not isinstance(x_dict, Dict):\n",
    "            raise RuntimeError(\"Arguments must be of type Dict\")\n",
    "\n",
    "        self.y_dict = y_dict\n",
    "        self.x_dict = x_dict\n",
    "\n",
    "        n_y = len(y_dict)\n",
    "        n_x = len(x_dict)\n",
    "        self.init_prob = np.zeros((n_y,), float) \n",
    "        self.transition_prob = np.zeros((n_y, n_y), float) \n",
    "        self.observation_prob = np.zeros((n_y, n_x), float) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Learning\n",
    "\n",
    "Compute the matrices of probabilities hmm.init_prob, hmm.observation_prob and hmm.transition_prob from the data.\n",
    "\n",
    "You **must** smooth the distributions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HMM(tags_dict, word_dict)\n",
    "\n",
    "###init prob###\n",
    "for sent in train_data:\n",
    "    tag = sent[0][1]\n",
    "    id_tag = tags_dict.str_to_id(tag)\n",
    "    hmm.init_prob[id_tag]+=1\n",
    "hmm.init_prob+=1\n",
    "hmm.init_prob/=(len(tags_dict)+len(train_data))\n",
    "\n",
    "\n",
    "###transition prob###\n",
    "d_tag = defaultdict(int)\n",
    "for sent in train_data:\n",
    "    for i in range(1,len(sent)):\n",
    "        cur_tag = sent[i][1]\n",
    "        pred_tag = sent[i-1][1]\n",
    "        id_cur_tag = tags_dict.str_to_id(cur_tag)\n",
    "        id_pred_tag = tags_dict.str_to_id(pred_tag)\n",
    "        hmm.transition_prob[id_pred_tag][id_cur_tag]+=1\n",
    "        d_tag[id_pred_tag]+=1\n",
    "        \n",
    "hmm.transition_prob+=1\n",
    "for id_tag in d_tag:\n",
    "    hmm.transition_prob[id_tag,:]/=(d_tag[id_tag]+len(tags_dict))   \n",
    "    \n",
    "###observation prob###\n",
    "d_tag = defaultdict(int)\n",
    "for sent in train_data:\n",
    "    for i in range(len(sent)):\n",
    "        cur_tag = sent[i][1]\n",
    "        cur_w = sent[i][0]\n",
    "        id_cur_tag = tags_dict.str_to_id(cur_tag)\n",
    "        id_cur_w = word_dict.str_to_id(cur_w)\n",
    "        hmm.observation_prob[id_cur_tag][id_cur_w]+=1\n",
    "        d_tag[id_cur_tag]+=1\n",
    "        \n",
    "hmm.observation_prob+=1\n",
    "for id_tag in d_tag:\n",
    "    hmm.observation_prob[id_tag,:]/=(d_tag[id_tag]+len(word_dict))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three cells check that the distribution you computed correctly sum to one. The first cell should output 1.0, the two others should output array containing twelve times the number 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.init_prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.transition_prob.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.observation_prob.sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. Viterbi\n",
    "\n",
    "Implement the viterbi **without** computing in the log domain. What tagging accuracy do you achieve? How is it compared to the naive Bayes model of last week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(hmm, words):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - hmm: an HMM object\n",
    "    - words: a list of words (ie a sentence)\n",
    "    Return:\n",
    "    - a list of POS tags\n",
    "    \"\"\"\n",
    "\n",
    "    chart = np.zeros((len(hmm.y_dict), len(words)), float)\n",
    "    backpointer = np.zeros((len(hmm.y_dict), len(words)), float)\n",
    "    for i in range(len(hmm.y_dict)):\n",
    "        id_w0 = word_dict.str_to_id(words[0])\n",
    "        chart[i,0] = hmm.init_prob[i]*hmm.observation_prob[i,id_w0]\n",
    "        \n",
    "    for i in range(1,len(words)):\n",
    "        for j in range(len(hmm.y_dict)):\n",
    "            b_score = -1.0\n",
    "            for k in range(len(hmm.y_dict)):\n",
    "                score = hmm.transition_prob[j,k]*hmm.observation_prob[j,i]*chart[k,i-1]\n",
    "                if(score>b_score):\n",
    "                    chart[j,i] = score\n",
    "                    b_score = score\n",
    "                    backpointer[j,i] = k\n",
    "    #print(chart)           \n",
    "    y = np.zeros(len(words))\n",
    "    y[len(words)-1] = np.argmax(chart[:,len(words)-1], axis=0)\n",
    "    \n",
    "    print(chart)\n",
    "    print(y)\n",
    "    for j in range(1,len(words))[::-1]:\n",
    "        y[j-1] = backpointer[int(y[j-2]),j]\n",
    "    pred = [tags_dict.id_to_str(int(i)) for i in (y)]\n",
    "    print(y)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_dict.id_to_str(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.48827851e-07 7.31243131e-08 2.89123138e-13 4.94855334e-16]\n",
      " [8.83851784e-07 3.02494725e-07 5.17741780e-10 7.56597151e-17]\n",
      " [2.63409992e-07 8.67638411e-08 2.96406369e-13 4.57502182e-12]\n",
      " [2.43788845e-07 5.28413238e-08 2.82911163e-12 5.94298264e-15]\n",
      " [9.20687901e-02 4.11947683e-09 3.47470702e-13 3.58938928e-17]\n",
      " [1.04005494e-06 2.21016739e-07 1.53197410e-12 1.33457129e-15]\n",
      " [4.06692240e-07 6.49123430e-09 5.70877953e-13 5.47216258e-16]\n",
      " [6.25914760e-07 7.20940783e-08 2.59895280e-13 3.88644654e-16]\n",
      " [1.14848182e-06 3.26308767e-07 5.22778481e-13 8.94773494e-16]\n",
      " [1.54934759e-06 1.14779890e-07 7.33280342e-13 1.25506278e-15]\n",
      " [3.05186913e-06 3.02629370e-08 1.16686735e-12 5.51200807e-16]\n",
      " [7.71170388e-07 5.69367132e-08 1.82886976e-12 3.13024397e-15]]\n",
      "[0. 0. 0. 2.]\n",
      "[4. 1. 1. 2.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DET', 'ADP', 'ADP', 'VERB']"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi(hmm,['the', 'cat', 'is', 'black'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging accuract: 20.93\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the HMM using the viterbi\n",
    "n_tags = 0\n",
    "n_correct_tags = 0\n",
    "for sentence in test_data:\n",
    "    words = [w for w, t in sentence]\n",
    "    pred = viterbi(hmm, words)\n",
    "    n_tags += len(sentence)\n",
    "    n_correct_tags += sum(1 for w in range(len(sentence))  if sentence[w][1] == pred[w])\n",
    "\n",
    "print(\"Tagging accuract: %.2f\" % (100 * n_correct_tags / n_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. Viterbi in the log domain\n",
    "\n",
    "Copy/paste you code from the previous cell and change it to compute in the log domain. What tagging accuracy do you achieve? How is it compared to the naive Bayes model of last week and to the previous implementation of the viterbi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_log(hmm, words):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - hmm: an HMM object\n",
    "    - words: a list of words (ie a sentence)\n",
    "    Return:\n",
    "    - a list of POS tags\n",
    "    \"\"\"\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the HMM using the viterbi in the log domain\n",
    "\n",
    "n_tags = 0\n",
    "n_correct_tags = 0\n",
    "for sentence in test_data:\n",
    "    words = [w for w, t in sentence]\n",
    "    pred = viterbi_log(hmm, words)\n",
    "    n_tags += len(sentence)\n",
    "    n_correct_tags += sum(1 for w in range(len(sentence))  if sentence[w][1] == pred[w])\n",
    "\n",
    "print(\"Tagging accuract: %.2f\" % (100 * n_correct_tags / n_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Marginalization\n",
    "\n",
    "As a last exercise, implement function that evaluate the probability of a sequence of words and a sequence of hidden states given a HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilit_y(hmm, tags):\n",
    "    #TODO..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = \"DET NOUN VERB DET ADJ NOUN .\".split()\n",
    "print(probabilit_y(hmm, tags))\n",
    "random.shuffle(tags)\n",
    "print(probabilit_y(hmm, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilit_x(hmm, words):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is a sentence .\".split()\n",
    "print(probabilit_x(hmm, sentence))\n",
    "random.shuffle(sentence)\n",
    "print(probabilit_x(hmm, sentence))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TC4-tp2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
