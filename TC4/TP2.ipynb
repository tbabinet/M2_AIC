{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GROUP : BERTELLI, BABINET, HAMIDI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IwILkW1F8FnJ"
   },
   "source": [
    "# TC4 - Lab exercises 2\n",
    "\n",
    "In this notebook, we are going to improve the POS tagger of last week. Instead of using a naive Bayes classifier, we will rely on a HMM where:\n",
    "- the hidden states are POS tags\n",
    "- the observations are words\n",
    "\n",
    "It is a first order HMM where probabilities are defined as follows:\n",
    "$$\n",
    "p(y_1...y_n, x_1...x_n) = p(y_1) \\prod_{i=2}^n p(y_i | y_{i-1}) \\prod_{i=1}^n p(x_i | y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IwILkW1F8FnJ"
   },
   "source": [
    "# 1. Data\n",
    "\n",
    "We first need to load and split the data between train and test data. You need to report the code from last week with the same split (90% train / 10% test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51606, 5734)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store the parameters of the HMM in numpy arrays. Therefore, to simplify the model, we rely on a dictionnary that maps words to tokens:\n",
    "- the constructor takes as argument an iteratable over strings (e.g. list of strings) containing the vocabulary to store in the dictionnary\n",
    "- you can set unk=\"\\*UNK\\*\" to add entry for unknown strings (do not do it for POS tags!)\n",
    "- len(dict) gives you the numbers of entry in the dict\n",
    "- str_to_id maps a string to an index\n",
    "- id_to_str gives you the string stored at a given index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dict:\n",
    "    def __init__(self, words, unk=None):\n",
    "        self._unk = unk\n",
    "        self._word_to_id = dict()\n",
    "        self._id_to_word = list()\n",
    "\n",
    "        if unk in words:\n",
    "            raise RuntimeError(\"UNK word exists in vocabulary\")\n",
    "\n",
    "        if unk is not None:\n",
    "            self.unk_index = self._add_word(unk)\n",
    "\n",
    "        for word in words:\n",
    "            self._add_word(word)\n",
    "\n",
    "    # for internal use only!\n",
    "    def _add_word(self, word):\n",
    "        if word not in self._word_to_id:\n",
    "            id = len(self._id_to_word)\n",
    "            self._word_to_id[word] = id\n",
    "            self._id_to_word.append(word)\n",
    "            return id\n",
    "        else:\n",
    "            return self._word_to_id[word]\n",
    "\n",
    "    def str_to_id(self, word):\n",
    "        if self._unk is not None:\n",
    "            return self._word_to_id.get(word, self.unk_index)\n",
    "        else:\n",
    "            return self._word_to_id[word]\n",
    "\n",
    "    def id_to_str(self, id):\n",
    "        return self._id_to_word[id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._word_to_id)\n",
    "\n",
    "    def has_unk(self):\n",
    "        return self._unk is not None\n",
    "    \n",
    "    def unk(self):\n",
    "        return self.unk_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N. entry:  4\n",
      "Index of \"b\": 1\n",
      "Index of \"e\": 0\n"
     ]
    }
   ],
   "source": [
    "test_dict = Dict([\"a\", \"b\", \"c\"], unk=\"*UNK*\")\n",
    "print(\"N. entry: \", len(test_dict))\n",
    "print(\"Index of \\\"b\\\":\", test_dict.str_to_id(\"a\"))\n",
    "# the following line does not throw an error because we gave a unk word\n",
    "print(\"Index of \\\"e\\\":\", test_dict.str_to_id(\"e\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build the dictionnary of words and tags. We will restrict the dictionnary of words to contain only words that appears 10 or more times in the training data (use the code of last time).\n",
    "\n",
    "For the dictionnary of words, set the unk parameters to any string you want. For the dictionnary of POS tags, do not set an unk word!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_per_word = {}\n",
    "distribution_per_word_correct = {}\n",
    "\n",
    "\n",
    "for s in train_data:\n",
    "    for w, tag in s:\n",
    "        if w in distribution_per_word : \n",
    "            if tag in distribution_per_word[w]:\n",
    "                distribution_per_word[w][tag]+=1\n",
    "            else:\n",
    "                distribution_per_word[w][tag] = 1\n",
    "        else:\n",
    "            distribution_per_word[w]={}\n",
    "            distribution_per_word[w][tag] = 1\n",
    "\n",
    "for w in distribution_per_word:\n",
    "    total = sum(distribution_per_word[w].values())\n",
    "    if (total>=10): \n",
    "        distribution_per_word_correct[w] = distribution_per_word[w]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [list(x.keys()) for x in distribution_per_word_correct.values()]\n",
    "\n",
    "tags = [item for sublist in tmp for item in sublist]\n",
    "\n",
    "word_dict = Dict(distribution_per_word_correct.keys(), unk = \"**UNK**\")\n",
    "tags_dict = Dict(tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hidden Markov Model\n",
    "\n",
    "The HMM class is a simple container for:\n",
    "- the dictionnary of hidden states y_dict (i.e. dictionnary of tags)\n",
    "- the dictionnary of observations x_dict (i.e. dictionnary of words)\n",
    "- the parameters of the HMM:\n",
    "    * init_prob $\\in \\mathbb R^{|Y|}$: initial tag probabilities $p(y_0) = init\\_prob[y_0]$\n",
    "    * transition_prob $\\in \\mathbb R^{|Y| \\times |Y|}$: tag transition probabilities $p(y_i | y_{i - 1}) = transition\\_prob[y_{i - 1}, y_i]$\n",
    "    * observation_prob $\\in \\mathbb R^{|Y| \\times |X|}$: observation probabilities $p(x_i | y_i) = observation\\_prob[y_i, x_i]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtP9d0Pz8FnL"
   },
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, y_dict, x_dict):\n",
    "        if not isinstance(y_dict, Dict) or not isinstance(x_dict, Dict):\n",
    "            raise RuntimeError(\"Arguments must be of type Dict\")\n",
    "\n",
    "        self.y_dict = y_dict\n",
    "        self.x_dict = x_dict\n",
    "\n",
    "        n_y = len(y_dict)\n",
    "        n_x = len(x_dict)\n",
    "        self.init_prob = np.zeros((n_y,), float) \n",
    "        self.transition_prob = np.zeros((n_y, n_y), float) \n",
    "        self.observation_prob = np.zeros((n_y, n_x), float) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Learning\n",
    "\n",
    "Compute the matrices of probabilities hmm.init_prob, hmm.observation_prob and hmm.transition_prob from the data.\n",
    "\n",
    "You **must** smooth the distributions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HMM(tags_dict, word_dict)\n",
    "\n",
    "###init prob###\n",
    "#computing the frequency of each tag to be the first tag of the sequence\n",
    "for sent in train_data:\n",
    "    tag = sent[0][1]\n",
    "    id_tag = tags_dict.str_to_id(tag)\n",
    "    hmm.init_prob[id_tag]+=1\n",
    "#smooting\n",
    "hmm.init_prob+=1\n",
    "hmm.init_prob/=(len(tags_dict)+len(train_data))\n",
    "\n",
    "\n",
    "###transition prob###\n",
    "#computing p(yi|y(i-1)) \n",
    "d_tag = defaultdict(int)\n",
    "for sent in train_data:\n",
    "    for i in range(1,len(sent)):\n",
    "        cur_tag = sent[i][1]\n",
    "        pred_tag = sent[i-1][1]\n",
    "        id_cur_tag = tags_dict.str_to_id(cur_tag)\n",
    "        id_pred_tag = tags_dict.str_to_id(pred_tag)\n",
    "        hmm.transition_prob[id_pred_tag][id_cur_tag]+=1\n",
    "        d_tag[id_pred_tag]+=1\n",
    "#smoothing\n",
    "hmm.transition_prob+=1\n",
    "for id_tag in d_tag:\n",
    "    hmm.transition_prob[id_tag,:]/=(d_tag[id_tag]+len(tags_dict))   \n",
    "    \n",
    "###observation prob###\n",
    "d_tag = defaultdict(int)\n",
    "for sent in train_data:\n",
    "    for i in range(len(sent)):\n",
    "        cur_tag = sent[i][1]\n",
    "        cur_w = sent[i][0]\n",
    "        id_cur_tag = tags_dict.str_to_id(cur_tag)\n",
    "        id_cur_w = word_dict.str_to_id(cur_w)\n",
    "        hmm.observation_prob[id_cur_tag][id_cur_w]+=1\n",
    "        d_tag[id_cur_tag]+=1\n",
    "#smoothing\n",
    "hmm.observation_prob+=1\n",
    "for id_tag in d_tag:\n",
    "    hmm.observation_prob[id_tag,:]/=(d_tag[id_tag]+len(word_dict))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three cells check that the distribution you computed correctly sum to one. The first cell should output 1.0, the two others should output array containing twelve times the number 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.init_prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.transition_prob.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.observation_prob.sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. Viterbi\n",
    "\n",
    "Implement the viterbi **without** computing in the log domain. What tagging accuracy do you achieve? How is it compared to the naive Bayes model of last week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(hmm, words):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - hmm: an HMM object\n",
    "    - words: a list of words (ie a sentence)\n",
    "    Return:\n",
    "    - a list of POS tags\n",
    "    \"\"\"\n",
    "\n",
    "    #DEFINING THE CHART AND BACKPOINTER TABLES\n",
    "    chart = np.zeros((len(hmm.y_dict), len(words)), float)\n",
    "    backpointer = np.zeros((len(hmm.y_dict), len(words)), float)\n",
    "    \n",
    "    #FILLING THE FIRST LINE OF THE CHART TABLE \n",
    "    for i in range(len(hmm.y_dict)):\n",
    "        id_w0 = word_dict.str_to_id(words[0])\n",
    "        chart[i,0] = hmm.init_prob[i]*hmm.observation_prob[i,id_w0]\n",
    "        \n",
    "    #FILLING BACKPOINTER TABLE AND THE REST OF THE CHART TABLE\n",
    "    #for each word\n",
    "    for i in range(1,len(words)):\n",
    "        id_w = word_dict.str_to_id(words[i])\n",
    "        #for each possible tag \n",
    "        for j in range(len(hmm.y_dict)):\n",
    "            b_score = -1.0\n",
    "            #for each possible (tag, tag') we want the maximum of the equation below\n",
    "            for k in range(len(hmm.y_dict)):\n",
    "                score = hmm.transition_prob[k,j]*hmm.observation_prob[j,id_w]*chart[k,i-1]\n",
    "                #if the score is superior, we update because it wasn't the maximun\n",
    "                if(score>b_score):\n",
    "                    chart[j,i] = score\n",
    "                    b_score = score\n",
    "                    backpointer[j,i] = k\n",
    "    \n",
    "    #FILLING THE TABLE CONTAINING THE ID OF THE GOOD TAGS\n",
    "    y = np.zeros(len(words))\n",
    "    y[len(words)-1] = np.argmax(chart[:,len(words)-1], axis=0)\n",
    "\n",
    "    for j in range(1,len(words))[::-1]:\n",
    "        y[j-1] = backpointer[int(y[j-2]),j]\n",
    "    #MAPPING EACH ID TAG TO THE TAG\n",
    "    pred = [tags_dict.id_to_str(int(i)) for i in (y)]\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PRON'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_dict.id_to_str(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET', 'NOUN', 'VERB', 'ADJ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi(hmm,['the', 'cat', 'is', 'black'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging accuract: 88.79\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the HMM using the viterbi\n",
    "n_tags = 0\n",
    "n_correct_tags = 0\n",
    "for sentence in test_data:\n",
    "    words = [w for w, t in sentence]\n",
    "    pred = viterbi(hmm, words)\n",
    "    n_tags += len(sentence)\n",
    "    n_correct_tags += sum(1 for w in range(len(sentence))  if sentence[w][1] == pred[w])\n",
    "\n",
    "print(\"Tagging accuract: %.2f\" % (100 * n_correct_tags / n_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER : \n",
    "Last week, with the naive bayes implementation, we got a maximun of 86% of accuracy, and now we get almost 89% of accuracy.\n",
    "So the viterbi and hmm model performs better than the naie Bayes model. This is quite logical, because last  week our model didn't take into account the words before, but just the word to predict the tag. Now our model takes into account the tag that precede the current words, and so performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. Viterbi in the log domain\n",
    "\n",
    "Copy/paste you code from the previous cell and change it to compute in the log domain. What tagging accuracy do you achieve? How is it compared to the naive Bayes model of last week and to the previous implementation of the viterbi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_log(hmm, words):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - hmm: an HMM object\n",
    "    - words: a list of words (ie a sentence)\n",
    "    Return:\n",
    "    - a list of POS tags\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    chart = np.zeros((len(hmm.y_dict), len(words)), float)\n",
    "    backpointer = np.zeros((len(hmm.y_dict), len(words)), float)\n",
    "    for i in range(len(hmm.y_dict)):\n",
    "        id_w0 = word_dict.str_to_id(words[0])\n",
    "        chart[i,0] = np.log(hmm.init_prob[i]) +np.log(hmm.observation_prob[i,id_w0])\n",
    "        \n",
    "    for i in range(1,len(words)):\n",
    "        id_w = word_dict.str_to_id(words[i])\n",
    "        for j in range(len(hmm.y_dict)):\n",
    "            b_score = -float('Inf')\n",
    "            for k in range(len(hmm.y_dict)):\n",
    "                score = np.log(hmm.transition_prob[k,j]) + np.log(hmm.observation_prob[j,id_w])+ (chart[k,i-1])\n",
    "                if(score>b_score):\n",
    "                    chart[j,i] = score\n",
    "                    b_score = score\n",
    "                    backpointer[j,i] = k\n",
    "    #print(chart)           \n",
    "    y = np.zeros(len(words))\n",
    "    y[len(words)-1] = np.argmax(np.exp(chart[:,len(words)-1]), axis=0)\n",
    "\n",
    "    for j in range(1,len(words))[::-1]:\n",
    "        y[j-1] = backpointer[int(y[j-2]),j]\n",
    "    pred = [tags_dict.id_to_str(int(i)) for i in (y)]\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging accuract: 88.79\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the HMM using the viterbi in the log domain\n",
    "\n",
    "n_tags = 0\n",
    "n_correct_tags = 0\n",
    "i =0\n",
    "for sentence in test_data:\n",
    "    words = [w for w, t in sentence]\n",
    "    pred = viterbi_log(hmm, words)\n",
    "    n_tags += len(sentence)\n",
    "    n_correct_tags += sum(1 for w in range(len(sentence))  if sentence[w][1] == pred[w])\n",
    "\n",
    "print(\"Tagging accuract: %.2f\" % (100 * n_correct_tags / n_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER: \n",
    "As before, for the \"normal\" viterbi, we get better results than the naive bayes of last week, and this for the same reason.\n",
    "We get the same result. Indeed, the log viterbi is usefull if we have paths (ie sequences of tags), that are large and so the resulting probabilities, which are the product, are almost equal to zero. Here, as the results are the same, we deduce, that the paths aren't described by a product which is too small.\n",
    "\n",
    "However, we should keep the log probabilities because it doesn't affect  badly the performance of our model, and if these products mentionned before tend to zero, it can make it more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Marginalization\n",
    "\n",
    "As a last exercise, implement function that evaluate the probability of a sequence of words and a sequence of hidden states given a HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilit_y(hmm, tags):\n",
    "    p = hmm.init_prob[tags_dict.str_to_id(tags[0])]\n",
    "    for i in range (1, len(tags)):\n",
    "        cur_id_tag = tags_dict.str_to_id(tags[i])\n",
    "        pred_id_tag = tags_dict.str_to_id(tags[i-1])\n",
    "        p *=  hmm.transition_prob[pred_id_tag][cur_id_tag]\n",
    "    return(p)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00015516963615024506\n",
      "1.7184983167278761e-06\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "tags = \"DET NOUN VERB DET ADJ NOUN .\".split()\n",
    "print(probabilit_y(hmm, tags))\n",
    "random.shuffle(tags)\n",
    "print(probabilit_y(hmm, tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER:\n",
    "\n",
    "So we can see that the initial structure which is quite a normal one, has a probability of 10^-4, and the other one, obtained after a random shuffle, has a probability of 10^-6.\n",
    "Thus, this is logical because the first structure is normal and the second one random. We can judge wether our structure, is logical or not. The higher the probability is the most likely the sequence is possible.\n",
    "\n",
    "Thus, it allows us to judge after how \"sure\" we are about the sequence of tags emitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilit_x(hmm, words):\n",
    "    chart = np.zeros((len(hmm.y_dict), len(words)), float)\n",
    "    for i in range (len(hmm.y_dict)):\n",
    "        chart[i,0] = hmm.init_prob[i] * hmm.observation_prob[i][word_dict.str_to_id(words[0])]\n",
    "\n",
    "    for j in range(1,len(words)):\n",
    "        for i in range(len(hmm.y_dict)):\n",
    "            for k in range(len(hmm.y_dict)):\n",
    "                chart[i,j]+= hmm.transition_prob[k][i]*hmm.observation_prob[k][word_dict.str_to_id(words[j])] * chart[k, j-1]\n",
    "\n",
    "    return(np.sum(chart[:,len(words)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0913331285286098e-15\n",
      "3.6403486468072695e-17\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This is a sentence .\".split()\n",
    "print(probabilit_x(hmm, sentence))\n",
    "random.shuffle(sentence)\n",
    "print(probabilit_x(hmm, sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ANSWER :\n",
    "\n",
    "In this case we observe that the probability of the real sentence is higher (100 x more) than the random sentence. Thus this confirms us our HMM model works"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TC4-tp2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
